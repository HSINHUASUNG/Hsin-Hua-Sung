---
title: "R Notebook"
output: html_notebook
---
```{r}
# Connect to Guardian API
library(guardianapi)
gu_api_key()
```

```{r}
# Get data include key words "police" OR "law enforcement"
data_all <-gu_content(query='"police" OR "law enforcement"',
        from_date="2021-01-01",
        to_date="2023-03-31")
```
```{r}
# Check the data
dim(data_all)
head(data_all)
# 24863 news articles collected
```

```{r}
# Limit the data to only section_name == 'UK news'
data_uk_sec <- subset(data_all, section_name == 'UK news')
dim(data_uk_sec)
head(data_uk_sec) 
```


```{r}
# Save as CSV file
library(dplyr)
my_data <- data_uk_sec %>% 
  select(-tags) # remove the non-informative column that contains nested lists
my_df <- as.data.frame(my_data)
write.csv(my_df, "raw_data.csv", row.names = TRUE)
```

```{r}
# Read collected data
my_data <- read.csv('raw_data.csv')
dim(my_data)# 3259 UK news articles 
```

```{r}
# Load required packages
library(tidyr)
library(quanteda)
library(caret)
library(dplyr)
library(tidytext)
library(syuzhet)
library(tidyverse)
library(quanteda.textstats)
library(ggplot2)
```

```{r}
# Pre-processing
# Select required columns
my_data_hbd <- my_data %>% 
  select(headline, body_text, first_publication_date)
# Remove duplicates
my_data_hbd   <- distinct(my_data_hbd)
# Remove noise
my_data_hbd$body_text <- gsub("\\W+", " ", my_data_hbd$body_text)
# Tokenise the data and remove punctuation, numbers, and symbols
# Convert all text to lowercase and apply stemming
my_data_tokens <- tokens(my_data_hbd$body_text, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>% tokens_remove(stopwords("english")) %>% tokens_tolower() %>% tokens_wordstem(language = quanteda_options("language_stemmer")) 
```

```{r}
dim(my_data_hbd)# 3194 UK news articles after pre-processing
head(my_data_hbd)
```


```{r}
## Bing lexicon
# Try to use "bing" lexicon to get sentiment score for each news article
# Convert the data to a tidy format
tokens <- my_data_hbd %>%
  unnest_tokens(word, body_text) %>%
  anti_join(stop_words)
# Load the lexicon
lexicon <- get_sentiments("bing")

# Join the lexicon with data
my_data_sentiment <- tokens %>%
  inner_join(lexicon)

# Calculate the sentiment score
my_data_sentiment_scores <- my_data_sentiment %>%
  count(headline, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(sentiment_score = positive - negative)

# Visualise the sentiment scores
ggplot(my_data_sentiment_scores, aes(x = sentiment_score)) +
  geom_histogram(binwidth = 2, color = "black", fill = "lightblue") +
  labs(title = "Distribution of Sentiment Scores",
       x = "Sentiment Score",
       y = "Frequency") +
  theme_minimal()

# Categorise sentiment scores as positive, negative, or neutral
my_data_sentiment_category <- my_data_sentiment_scores %>%
  mutate(sentiment_category = ifelse(sentiment_score > 0, "Positive",
                                     ifelse(sentiment_score < 0, "Negative", "Neutral")))
  
# Count the number of articles in each sentiment category
my_data_sentiment_counts <- my_data_sentiment_category %>%
  count(sentiment_category)
print(my_data_sentiment_counts)
```

```{r}
# Visualise the distribution of sentiment categories in articles
ggplot(data = my_data_sentiment_counts, aes(x = sentiment_category, y = n, fill = sentiment_category)) +
  geom_col() +
  geom_text(aes(label = n), position = position_stack(vjust = 0.5), color = "black", size = 4) +
  labs(x = "Sentiment Category", y = "Number of Articles", fill = "Sentiment Category") +
  ggtitle("Sentiment Categories Proportions Based on Bing Lexicon")

```

```{r}
# Try to explore sentiment change over time based on Bing lexicon
# Calculate the sentiment score by date
sentiment_scores_date <- my_data_sentiment %>%
  count(first_publication_date, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
  mutate(sentiment_score = positive - negative)

# Categorise sentiment scores as positive, negative, or neutral
sentiment_category_date <- sentiment_scores_date %>%
  mutate(sentiment_category = ifelse(sentiment_score > 0, "Positive",
                                     ifelse(sentiment_score < 0, "Negative", "Neutral")))

# Convert date to date object
sentiment_category_date$first_publication_date <- as.Date(sentiment_category_date$first_publication_date)

# Visualise the sentiment scores over time
ggplot(sentiment_category_date, aes(x = first_publication_date, y = sentiment_score)) +
  geom_line(color = "#000080") +
  geom_hline(yintercept = 0, color = "red") +
  labs(title = "Sentiment Score Over Time Based on Bing Lexicon",
       x = "Date",
       y = "Sentiment Score") +
  theme_minimal()
```


```{r}
## Use get_sentiment() from syuzhet package to get sentiment score for each news article
library(syuzhet)
# Calculate the sentiment scores
my_data_hbd$sentiment_score <- get_sentiment(my_data_hbd$body_text)

# Categorise sentiment scores as positive, negative, or neutral
my_data_hbd$sentiment_category <- cut(my_data_hbd$sentiment_score, breaks=c(-Inf,-0.1,0.1,Inf), labels=c("Negative", "Neutral", "Positive"))

# Visualise the distribution of sentiment scores
ggplot(my_data_hbd, aes(x=sentiment_score, fill=sentiment_category)) +
  geom_histogram(binwidth=0.05, alpha=0.5, position="identity") +
  labs(title="Sentiment Scores Distribution", x="Sentiment Score", y="Count") +
  theme_minimal()
```

```{r}
# Visualise the proportion of positive, negative, and neutral sentiment categories
ggplot(my_data_hbd, aes(x = sentiment_category, fill = sentiment_category)) +
  geom_bar() +
  geom_text(stat = "count", aes(label = after_stat(count)), 
            position = position_stack(vjust = 0.5)) +
  labs(title = "Sentiment Categories Proportions Based on Syuzhet Package",
       x = "Sentiment Category", y = "Number of Articles") +
  theme_minimal() +
  theme(panel.background = element_rect(fill = "grey90", color = NA),
        panel.grid.major = element_line(color = "white"),
        panel.grid.minor = element_blank())

```


```{r}
# Try to explore sentiment change over time based on syuzhet package
# Convert first_publication_date to a date object
my_data_hbd$first_publication_date <- as.Date(my_data_hbd$first_publication_date)

# Aggregate sentiment score by date
sentiment_by_date <- my_data_hbd %>%
  group_by(first_publication_date) %>%
  summarise(sentiment_score = mean(sentiment_score))

# Create a line chart of sentiment score over time
ggplot(sentiment_by_date, aes(x=first_publication_date, y=sentiment_score)) +
  geom_line(color = "#000080") +
  geom_hline(yintercept = 0, color = "red") +
  labs(title="Sentiment Score over Time Based on Syuzhet Package", x="Date", y="Sentiment Score") +
  theme_minimal()
```


```{r}
# Plot the sentiment score over time together (Bing Lexicon and Syuzhet Package)
library(patchwork)
library(scales)

# Create the first plot
plota <- ggplot(sentiment_category_date, aes(x = first_publication_date, y = sentiment_score)) +
  geom_line(color = "#000080") +
  geom_hline(yintercept = 0, color = "red") +
  labs(title = "Sentiment Score Over Time Based on Bing Lexicon",
       x = "Date",
       y = "Sentiment Score") +
  theme_minimal() +
  scale_x_date(date_breaks = "2 months", date_labels = "%b %y")

# Create the second plot
plotb <- ggplot(sentiment_by_date, aes(x=first_publication_date, y=sentiment_score)) +
  geom_line(color = "#000080") +
  geom_hline(yintercept = 0, color = "red") +
  labs(title="Sentiment Score over Time Based on Syuzhet Package", x="Date", y="Sentiment Score") +
  theme_minimal() +
  scale_x_date(date_breaks = "2 months", date_labels = "%b %y")

# Combine the plots vertically using patchwork
plota / plotb
```


```{r}
## Supervised Machine Learning
# Try to apply machine learning technique, build a model to predict sentiment
# Map the values of the original column "sentiment_category" to integers
my_data_ml <- my_data_hbd %>%
  mutate(id = row_number(), # add a new column with row numbers
         sentiment_category = case_when(
           sentiment_category == "Positive" ~ 2,
           sentiment_category == "Negative" ~ 1,
           sentiment_category == "Neutral" ~ 0,
           TRUE ~ NA_real_ # to handle any other cases
         )) 
```

```{r}
# Extract unigrams, bigrams as features
unigrams <- my_data_ml$body_text %>%
          tokens(remove_punct = T) %>%
          tokens_ngrams(1) %>%
          dfm()
bigrams <-  my_data_ml$body_text %>%
          tokens(remove_punct = T) %>%
          tokens_ngrams(2) %>%
          dfm()
# Apply sparsity correction to reduce the zero counts
unigrams_corrected <- dfm_trim(unigrams, sparsity = .90)
bigrams_corrected <- dfm_trim(bigrams, sparsity = .90)

# Check dimensions
dim(unigrams_corrected)

# TFIDF-weighted ngrams
unigrams_tfidf <- dfm_tfidf(unigrams_corrected,
                           scheme_tf = 'count',
                           scheme_df = 'inverse')
bigrams_tfidf <- dfm_tfidf(bigrams_corrected,
                          scheme_tf = 'count',
                          scheme_df = 'inverse')

# Convert ngrams to data frames
news_unigrams <- convert(unigrams_tfidf, 'data.frame')
news_bigrams <- convert(bigrams_tfidf, 'data.frame')

# Add id column to ngrams to join unigrams and bigrams later
news_unigrams$id <- my_data_ml$id
news_bigrams$id <- my_data_ml$id

# Remove doc_id column
unigrams_tfidf_df <- subset(news_unigrams, select = -c(doc_id))
bigrams_tfidf_df <- subset(news_bigrams, select = -c(doc_id))

# Join the two datasets
ml_data <- bigrams_tfidf_df %>% left_join(unigrams_tfidf_df, by = "id")
```

```{r}
# Get sentiment scores as features
ml_data$sentiment_score <- get_sentiment(my_data_ml$body_text)
# Add label column
ml_data$label <- my_data_ml$sentiment_category
# Remove the id column
ml_data <- ml_data %>% select(-id)
# Map labels
ml_data$label <- ifelse(ml_data$label == "2", "Positive", 
                   ifelse(ml_data$label == "1", "Negative", "Neutral")) 
# Convert label to factor
ml_data$label <- as.factor(ml_data$label) # convert label to factor
```

```{r}
# Apply machine learning setting
library(caret)
set.seed(123)
for_training <- createDataPartition(y = ml_data$label,
                                  p = .8,
                                  list = FALSE)

training_data <- ml_data[for_training,]
testing_data <- ml_data[-for_training,]
```


```{r}
# Set training control parameters
training_controls <- trainControl(method="cv", 
                                 number = 5,
                                 classProbs = T)
```

```{r}
## Train the SVM model
model.svm <- train(label ~ .,
                  data = training_data,
                  method = "svmLinear",
                  trControl = training_controls,
                  preProcess = c('zv')
                 )
```


```{r}
# Predict on the testing data
pred_svm <- predict(model.svm, testing_data)

# Evaluate the model
confusionMatrix(pred_svm, as.factor(testing_data$label), mode="prec_recall")
```


```{r}
## Train the Naive Bayes model
model.nb <- train(label ~ .,
                  data = training_data,
                  method = "naive_bayes",
                  trControl = training_controls,
                  preProcess = c('zv'))
```


```{r}
# Predict on the testing data
pred_nb <- predict(model.nb, testing_data)

# Evaluate the model
confusionMatrix(pred_nb, as.factor(testing_data$label), mode="prec_recall")
```

```{r}
# Set training control parameters
training_control <- trainControl(method = "cv", 
                                 number = 5, 
                                 verboseIter = FALSE)
```

```{r}
# Train the Neural Network (NN) model
model.nn <- train(label ~ .,
                  data = training_data,
                  trControl = training_control,
                  method = "nnet",
                  MaxNWts = 10000,
                  maxit = 10)
```

```{r}
# Predict on the testing data
pred_nn <- predict(model.nn, testing_data)
# Evaluate the model
confusionMatrix(pred_nn, testing_data$label, mode = "prec_recall")
```


```{r}
## Unsupervised Machine Learning
# Try to apply k-means clustering model using the extracted features
# Use ngrams frequencies as features
ml_grams <- bigrams_tfidf_df %>% left_join(unigrams_tfidf_df, by = "id")
# Remove the id column
ml_grams <- ml_grams %>% select(-id)
```

```{r}
# Use the k-means model on n-grams
# Determine k
wss <- numeric()
for(i in 1:10){
  ngrams_kmeans_temp <- kmeans(x = ml_grams,
                             centers = i,
                             iter.max = 20,
                             nstart = 10)
  wss[i] <- ngrams_kmeans_temp$tot.withinss
}
{plot(wss, type='b')} 

```

```{r}
# Settle for k=2
ngrams_kmeans <- kmeans(ml_grams,
                      centers = 2,
                      iter.max = 10,
                      nstart = 10)
```

```{r}
# Look at the clusters by their most prominent ngram differences
# Cluster 1
cat("Cluster 1: \n")
print(head(sort(ngrams_kmeans$centers[1, ], decreasing = TRUE), n = 20))
# Cluster 2
cat("\nCluster 2: \n")
print(head(sort(ngrams_kmeans$centers[2, ], decreasing = TRUE), n = 20))
```

```{r}
# Use the k-means model on sentiment score
# Determine k
wss <- numeric()
for(i in 1:10){
  sentiment_kmeans_temp <- kmeans(x = ml_data$sentiment_score,
                             centers = i,
                             iter.max = 10,
                             nstart = 10)
  wss[i] <- sentiment_kmeans_temp$tot.withinss
}
{plot(wss, type='b')}
```


```{r}
# Settle for k=3
sentiment_kmeans <- kmeans(ml_data$sentiment_score,
                      centers = 3,
                      iter.max = 5,
                      nstart = 5)
```


```{r}
# Print the centers of each sentiment cluster
print(sentiment_kmeans$centers[1, ])
print(sentiment_kmeans$centers[2, ])
print(sentiment_kmeans$centers[3, ])
```

```{r}
# Visualise the clusters
# Add the cluster labels to the original data frame
ml_data$cluster <- factor(sentiment_kmeans$cluster)

# Create the boxplot
ggplot(ml_data, aes(x = cluster, y = sentiment_score)) +
  geom_boxplot() +
  xlab("Cluster") +
  ylab("Sentiment Score")

```


```{r}
### Topic Modelling
# Load required packages
library(tidyr)
library(quanteda)
library(caret)
library(dplyr)
library(tidytext)
library(topicmodels)
library(ldatuning)
library(stm)
library(wordcloud)
library(tm)
library(ggplot2)
```

```{r}
# Prepare the data and select required columns
my_data_hbd <- my_data %>% 
  select(headline, body_text, first_publication_date)
# Remove duplicates
my_data_hbd <- distinct(my_data_hbd)
# Remove any noise in the data
my_data_hbd$body_text <- gsub("\\W+", " ", my_data_hbd$body_text)
```


```{r}
# Tokenise the data and remove punctuation, numbers, and symbols
# Convert all text to lowercase, apply stemming, and remove non-informative words for topic modelling purposes
tm_tokens <- tokens(my_data_hbd$body_text, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>% tokens_remove(stopwords("english")) %>% tokens_tolower() %>% tokens_wordstem(language = quanteda_options("language_stemmer")) %>% tokens_remove(c("polic", "offic", "peopl", "year", "day", "will", "also", "t", "s", "can", "uk", "british"))
```

```{r}
# Create a Document Feature Matrix (dfm)
dfm_tm <- dfm(tm_tokens)

# Trim the dfm for frequently and rarely occurring terms
dfm.trim <- dfm_trim(dfm_tm, min_docfreq = 0.075, max_docfreq = 0.9, docfreq_type = "prop")
dfm.trim
```

```{r}
## LDA topic modelling
n.topics <- 5
dfm2topicmodels <- convert(dfm.trim, to = "topicmodels")
lda.model <- LDA(dfm2topicmodels, n.topics, control = list(seed=111))
lda.model
```

```{r}
# Extract the topic-term matrix (beta) from the LDA model
beta_topics <- tidy(lda.model, matrix = "beta")
beta_topics
```

```{r}
# Group the terms by topics
beta_top_terms <- beta_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  arrange(topic, -beta)
```

```{r}
# Display the group terms 
beta_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

```{r}
# Top 10 terms for each topics 
as.data.frame(terms(lda.model, 10))

# List the topic number for each news article in the corpus.
data.frame(Topic = topics(lda.model))
```

```{r}
# Convert the topic model to a data frame
topics_df <- data.frame(topics(lda.model))

# Plot the count of topics
ggplot(topics_df, aes(x = factor(topics(lda.model)))) +
  geom_bar(aes(fill = factor(topics(lda.model))), alpha = 0.8) +
  scale_fill_discrete(name = "Topic") +
  labs(title = "Topic Distribution Based on Latent Dirichlet Allocation (LDA)", x = "Topic", y = "Count") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title = element_text(size = 12),
        panel.background = element_rect(fill = "white"),
        panel.grid.major = element_line(colour = "gray90")) +
  geom_text(stat='count', aes(label=..count..), vjust=-0.5)

```



```{r}
## Structural Topic Modeling (STM)
n.topics <- 5
dfm2stm <- convert(dfm.trim, to = "stm")
modell.stm <- stm(dfm2stm$documents, dfm2stm$vocab, K = n.topics, data = dfm2stm$meta, init.type = "Spectral")
```

```{r}
# Top 10 terms for each topic
as.data.frame(t(labelTopics(modell.stm, n = 10)$prob))

# Generate a word cloud for the first topic
par(mar=c(0.5, 0.5, 0.5, 0.5))
cloud(modell.stm, topic = 1, scale = c(2.25,.5))

# Plot a summary of the estimated topic shares for the corpus
plot(modell.stm, type = "summary", text.cex = 0.5, main = "Topic shares on the corpus as a whole", xlab = "estimated share of topics")
```


```{r}
# Print the top 5 terms for each of the 5 topics
labelTopics(modell.stm,topics = c(1:5), n=10)

# Plot histograms of the estimated topic shares within the documents for 3 randomly selected topics
plot(modell.stm, type = "hist", topics = sample(1:n.topics, size = 3), main = "histogram of the topic shares within the documents")

# Plot the top terms for each of the 5 topics
plot(modell.stm, type = "labels", topics = c(1,2,3,4,5), main = "Topic terms")
```

```{r}
library(ggplot2)
library(dplyr)
# Identify the most probable topic for each document
top_topics <- apply(modell.stm$theta, 1, which.max)
# Add the topic labels to the original data frame
my_data_hbd$Topic_stm <- top_topics
# Calculate the count of each topic
df_topic_counts <- my_data_hbd %>%
  group_by(Topic_stm) %>%
  summarize(count = n())

# Create the plot
ggplot(df_topic_counts, aes(x = factor(Topic_stm), y = count, fill = factor(Topic_stm))) +
  geom_bar(stat = "identity", alpha = 0.8) +
  scale_fill_discrete(name = "Topic") +
  labs(title = "Topic Distribution Based on Structural Topic Modeling (STM)", x = "Topic", y = "Count") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title = element_text(size = 12),
        panel.background = element_rect(fill = "white"),
        panel.grid.major = element_line(colour = "gray90")) +
  geom_text(aes(label = count), vjust=-0.5)

```



```{r}
## To see how the topics changed over time (LDA model)

# Add the topic labels to the original data frame
my_data_hbd$Topic_lda <- topics(lda.model)

# Convert the date column to a Date object
my_data_hbd$first_publication_date <- as.Date(my_data_hbd$first_publication_date, format = "%Y-%m-%d")

# Calculate the proportion of each topic by year
topic_proportions <- my_data_hbd %>%
  group_by(Topic_lda, year = lubridate::year(first_publication_date)) %>%
  summarise(count = n(), .groups = 'drop') %>%
  ungroup() %>%
  group_by(year) %>%
  mutate(proportion = count / sum(count))

# Plot the topic proportions over time
ggplot(topic_proportions, aes(x = year, y = proportion, color = factor(Topic_lda))) +
  geom_line() +
  labs(x = "Year", y = "Proportion", color = "Topic") +
  theme_bw()

# Calculate the proportion of each topic by half-year
topic_proportions_half_year <- my_data_hbd %>%
  group_by(Topic_lda, year = lubridate::year(first_publication_date), half_year = lubridate::floor_date(first_publication_date, unit = "6 months")) %>%
  summarise(count = n(), .groups = 'drop') %>%
  ungroup() %>%
  group_by(year, half_year) %>%
  mutate(proportion = count / sum(count))

# Plot the topic proportions over time
ggplot(topic_proportions_half_year, aes(x = half_year, y = proportion, color = factor(Topic_lda))) +
  geom_line() +
  labs(x = "Half-Year", y = "Proportion", color = "Topic") +
  theme_bw()
```



```{r}
## To see how the topics changed over time (STM model)

# Identify the most probable topic for each document
top_topics <- apply(modell.stm$theta, 1, which.max)
# Add the topic labels to the original data frame
my_data_hbd$Topic_stm <- top_topics

# Calculate the proportion of each topic by year
topic_proportions_stm <- my_data_hbd %>%
  group_by(Topic_stm, year = lubridate::year(first_publication_date)) %>%
  summarise(count = n(), .groups = 'drop') %>%
  ungroup() %>%
  group_by(year) %>%
  mutate(proportion = count / sum(count))

# Plot the topic proportions over time
ggplot(topic_proportions_stm, aes(x = year, y = proportion, color = factor(Topic_stm))) +
  geom_line() +
  labs(x = "Year", y = "Proportion", color = "Topic") +
  theme_bw()

# Calculate the proportion of each topic by half-year
topic_proportions_stm_half_year <- my_data_hbd %>%
  group_by(Topic_stm, year = lubridate::year(first_publication_date), half_year = lubridate::floor_date(first_publication_date, unit = "6 months")) %>%
  summarise(count = n(), .groups = 'drop') %>%
  ungroup() %>%
  group_by(year, half_year) %>%
  mutate(proportion = count / sum(count))


# Plot the topic proportions over time
ggplot(topic_proportions_stm_half_year, aes(x = half_year, y = proportion, color = factor(Topic_stm))) +
  geom_line() +
  labs(x = "Half-Year", y = "Proportion", color = "Topic") +
  theme_bw()
```

```{r}
# Refine the graphs, add the top 5 terms for each topic to the legend (LDA model)

# Extract the top 5 terms for each topic
top_terms <- terms(lda.model, 5)

# Create label for color legend
legend_label <- paste("Top 5 Terms\n",
                      "1: ", paste(top_terms[,1], collapse = ", "), "\n",
                      "2: ", paste(top_terms[,2], collapse = ", "), "\n",
                      "3: ", paste(top_terms[,3], collapse = ", "), "\n",
                      "4: ", paste(top_terms[,4], collapse = ", "), "\n",
                      "5: ", paste(top_terms[,5], collapse = ", "), sep = "")

# Plot the topic proportions over time
ggplot(topic_proportions_half_year, aes(x = half_year, y = proportion, color = factor(Topic_lda))) +
  geom_line() +
  labs(x = "Half-Year", y = "Proportion", 
       color = legend_label,
       subtitle = "Topic Changes Over Time Using LDA Topic Modelling") +
  theme_bw()

```


```{r}
# Refine the graphs, add the top 5 terms for each topic to the legend (STM model)

# Extract the top 5 terms for each topic
top_terms_stm <- labelTopics(modell.stm, topics = c(1:5), n=5)
top_terms_stm$frex

# Create label for color legend
legend_label_stm <- paste("Top 5 Terms\n",
                      "1: ", paste(top_terms_stm$frex[1,], collapse = ", "), "\n",
                      "2: ", paste(top_terms_stm$frex[2,], collapse = ", "), "\n",
                      "3: ", paste(top_terms_stm$frex[3,], collapse = ", "), "\n",
                      "4: ", paste(top_terms_stm$frex[4,], collapse = ", "), "\n",
                      "5: ", paste(top_terms_stm$frex[5,], collapse = ", "), sep = "")

# Plot the topic proportions over time
ggplot(topic_proportions_stm_half_year, aes(x = half_year, y = proportion, color = factor(Topic_stm))) +
  geom_line() +
  labs(x = "Half-Year", y = "Proportion", 
       color = legend_label_stm,
       subtitle = "Topic Changes Over Time Using Structural Topic Modeling (STM)") +
  theme_bw()

```

```{r}
# Try to combine sentiment and topic changes over time
library(patchwork)

# Create a line chart of sentiment score over time
p1 <- ggplot(sentiment_by_date, aes(x = first_publication_date, y = sentiment_score)) +
  geom_line() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue") +
  labs(title = "Sentiment Score over Time Based on Syuzhet Package", x = "Date", y = "Sentiment Score") +
  theme_minimal()+
  theme(plot.title = element_text(size = 11))

# Plot the topic proportions over time using LDA
p2 <- ggplot(topic_proportions_half_year, aes(x = half_year, y = proportion, color = factor(Topic_lda))) +
  geom_line() +
  labs(x = "Half-Year", y = "Proportion", 
       color = "Topic", subtitle = "Topic Changes Over Time Using LDA Topic Modelling") +
  theme_bw()

# Plot the topic proportions over time using STM
p3 <- ggplot(topic_proportions_stm_half_year, aes(x = half_year, y = proportion, color = factor(Topic_stm))) +
  geom_line() +
  labs(x = "Half-Year", y = "Proportion", 
       color = "Topic",
       subtitle = "Topic Changes Over Time Using Structural Topic Modeling") +
  theme_bw()

# Combine the three plots vertically
p1 + p2 + p3 + plot_layout(ncol = 1)

```







